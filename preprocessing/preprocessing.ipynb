{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import dateparser\n",
    "from cleanco import prepare_terms, basename\n",
    "import unidecode\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-45df49261184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/apps_matching_80%.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/apps_matching_80%.csv\", low_memory=False, lineterminator='\\n')\n",
    "df = df.drop(df[df[\"description\"].isnull()].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df, isTrain):\n",
    "        self.df_apps_match = df\n",
    "        self.df_after_preprocessing = pd.DataFrame()\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def pipeline(self):\n",
    "        self.add_non_processed()\n",
    "        self.preprocessing_maincategory()\n",
    "        self.preprocessing_titles()\n",
    "        self.preprocessing_author()\n",
    "        self.preprocessing_devsite()\n",
    "        self.preprocessing_description()\n",
    "        # self.preprocessing_releasedate()\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.train_test_split()\n",
    "            self.create_false_data()\n",
    "            self.save_csvs()\n",
    "            return\n",
    "        \n",
    "\n",
    "\n",
    "        print('pipeline done')\n",
    "\n",
    "    def add_non_processed(self):\n",
    "        print('add_non_processed')\n",
    "\n",
    "        self.df_after_preprocessing[\"id\"] = self.df_apps_match[\"id\"]\n",
    "        self.df_after_preprocessing[\"store\"] = self.df_apps_match[\"store\"]\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.df_after_preprocessing[\"id_matched\"] = self.df_apps_match[\"id_matched\"]\n",
    "\n",
    "    def preprocessing_maincategory(self):\n",
    "        print('preprocessing_maincategory')\n",
    "        \n",
    "        maincategory = pd.read_json('maincategory.json')\n",
    "        # Change from apple catagories ids to string catagories\n",
    "\n",
    "        self.df_after_preprocessing[\"apple_maincategory\"] = (\n",
    "            self.df_apps_match[self.df_apps_match[\"store\"] == 1]\n",
    "            .loc[:, \"maincategory\"]\n",
    "            .replace(\n",
    "                maincategory['apple']['numbered'],\n",
    "                maincategory['apple']['labeled'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Change from google play catagories to apple catagories\n",
    "        self.df_after_preprocessing[\"google_maincategory\"] = (\n",
    "            self.df_apps_match[self.df_apps_match[\"store\"] == 0]\n",
    "            .loc[:, \"maincategory\"]\n",
    "            .replace(\n",
    "                maincategory['google']['upper'],\n",
    "                maincategory['google']['lower'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def preprocessing_titles(self):\n",
    "        print('preprocessing_titles')\n",
    "\n",
    "        # lower case the titles and seperate the title\n",
    "        def create_title(titles):\n",
    "            # todo: ask davis if need it also for athuor\n",
    "            return [\n",
    "                title.lower()\n",
    "                .strip()\n",
    "                .partition(\":\")[0]\n",
    "                .partition(\"-\")[0]\n",
    "                .partition(\" \")[0]\n",
    "                for title in titles\n",
    "            ]\n",
    "\n",
    "        self.df_after_preprocessing[\"title\"] = create_title(self.df_apps_match[\"title\"])\n",
    "\n",
    "    def preprocessing_author(self):\n",
    "        print('preprocessing_author')\n",
    "\n",
    "        def create_author(authors):\n",
    "            terms = prepare_terms()\n",
    "            # Running twice in order to remove multiple endings, i.e Co., Ltd.\n",
    "            authors = [\n",
    "                basename(\n",
    "                    author.lower().strip(), terms, prefix=True, middle=True, suffix=True\n",
    "                )\n",
    "                for author in authors\n",
    "            ]\n",
    "            authors = [\n",
    "                basename(\n",
    "                    author, terms, prefix=True, middle=True, suffix=True\n",
    "                ).partition(\" \")[0]\n",
    "                for author in authors\n",
    "            ]\n",
    "            return authors\n",
    "\n",
    "        self.df_after_preprocessing[\"author\"] = create_author(self.df_apps_match[\"author\"])\n",
    "    \n",
    "    def preprocessing_devsite(self):\n",
    "        print('preprocessing_devsite')\n",
    "\n",
    "        def create_devsite(devsites):\n",
    "            return [\n",
    "                tldextract.extract(devsite.lower().strip()).domain\n",
    "                for devsite in devsites\n",
    "            ]\n",
    "\n",
    "        self.df_after_preprocessing[\"devsite\"] = create_devsite(self.df_apps_match[\"devsite\"].values.astype(str))\n",
    "\n",
    "    def preprocessing_releasedate(self):\n",
    "        print('preprocessing_releasedate')\n",
    "\n",
    "        def parse_date(date):\n",
    "            if not isinstance(date, str):\n",
    "                # always nan values\n",
    "                return\n",
    "\n",
    "            return dateparser.parse(date)\n",
    "\n",
    "        self.google_play_df_after_eda[\"releasedate\"] = pd.to_datetime(\n",
    "            self.google_play_df[\"releasedate\"].apply(parse_date), errors=\"coerce\"\n",
    "        )\n",
    "        # self.google_play_df['releasedate'].apply(parse_date).values.astype('datetime64[D]')\n",
    "        self.app_store_df_after_eda[\"releasedate\"] = pd.to_datetime(\n",
    "            self.app_store_df[\"releasedate\"].apply(parse_date), errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    def preprocessing_description(self):  # todo: make it better..\n",
    "        print('preprocessing_description')\n",
    "\n",
    "        def create_descriptions(descriptions):\n",
    "            return [\n",
    "                unidecode.unidecode(re.sub(r\"\\d+\", \"\", description))\n",
    "                .lower()\n",
    "                .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "                .strip()\n",
    "                for description in descriptions\n",
    "            ]\n",
    "\n",
    "        def save_tfidf_embeddings(documents):\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            embeddings = vectorizer.fit_transform(documents)\n",
    "            scipy.sparse.save_npz('../data/tfidf/embeddings.npz', embeddings)\n",
    "            \n",
    "\n",
    "        self.df_after_preprocessing[\"description\"] = create_descriptions(self.df_apps_match[\"description\"])\n",
    "        save_tfidf_embeddings(self.df_after_preprocessing[\"description\"])\n",
    "\n",
    "    def train_test_split(self):\n",
    "        print('train_test_split')\n",
    "        \n",
    "        # Shuffle dataset \n",
    "        shuffle_df = self.df_after_preprocessing.sample(frac=1)\n",
    "\n",
    "        # get 10% of data\n",
    "        test_size = int(0.1 * len(self.df_after_preprocessing))\n",
    "\n",
    "        test_set_first_part = shuffle_df[:test_size]\n",
    "        test_set_second_part = self.df_after_preprocessing[self.df_after_preprocessing[\"id_matched\"].isin(test_set_first_part[\"id\"])]\n",
    "\n",
    "        self.test_data = pd.concat([test_set_first_part, test_set_second_part])\n",
    "        self.train_data = self.df_after_preprocessing[~self.df_after_preprocessing[\"id\"].isin(self.test_data[\"id\"])]\n",
    "\n",
    "        self.google_play_test_data = self.test_data[self.test_data[\"store\"] == 0].rename(columns={'google_maincategory': 'maincategory'}).drop(columns=['store', 'apple_maincategory']).reset_index(drop=True)\n",
    "        \n",
    "        self.google_play_train_data = self.train_data[self.train_data[\"store\"] == 0].rename(columns={'google_maincategory': 'maincategory'}).drop(columns=['store', 'apple_maincategory']).reset_index(drop=True)\n",
    "\n",
    "        self.app_store_test_data = self.test_data[self.test_data[\"store\"] == 1].rename(columns={'apple_maincategory': 'maincategory'}).drop(columns=['store', 'google_maincategory']).reset_index(drop=True)\n",
    "\n",
    "        self.app_store_train_data = self.train_data[self.train_data[\"store\"] == 1].rename(columns={'apple_maincategory': 'maincategory'}).drop(columns=['store', 'google_maincategory']).reset_index(drop=True)\n",
    "\n",
    "        self.matched_test_data = self.test_data.merge(self.test_data, how=\"inner\", left_on=\"id\", right_on=\"id_matched\").reset_index(drop=True)\n",
    "        self.matched_train_data = self.train_data.merge(self.train_data, how=\"inner\", left_on=\"id\", right_on=\"id_matched\").reset_index(drop=True)\n",
    "\n",
    "        # remove duplicate matches \n",
    "        mask_test = self.matched_test_data[self.matched_test_data[\"store_x\"] == 1].index\n",
    "        self.matched_test_data.drop(mask_test, inplace=True)\n",
    "\n",
    "        mask_train = self.matched_train_data[self.matched_train_data[\"store_x\"] == 1].index\n",
    "        self.matched_train_data.drop(mask_train, inplace=True)\n",
    "\n",
    "        # remove unmatched apps\n",
    "        # TODO: check how come we have unmatched apps\n",
    "        self.matched_test_data = self.matched_test_data.dropna(subset=[\"id_y\"])\n",
    "        self.matched_train_data = self.matched_train_data.dropna(subset=[\"id_y\"])\n",
    "\n",
    "\n",
    "    def create_false_data(self):\n",
    "        print('create_false_data')\n",
    "\n",
    "        def get_false_data(apple_train_data, google_train_data):\n",
    "            num_of_matches = len(apple_train_data) if len(apple_train_data) % 2 == 0 else len(apple_train_data) - 1 # keeping it even\n",
    "            sample_size = int(num_of_matches * 5)\n",
    "\n",
    "            google_rand_indexes = np.random.randint(num_of_matches, size=int(sample_size / 2))\n",
    "            apple_rand_indexes = np.random.randint(num_of_matches, size=int(sample_size / 2))\n",
    "\n",
    "            self.google_random_rows = google_train_data.iloc[google_rand_indexes]\n",
    "            self.apple_random_rows = apple_train_data.iloc[apple_rand_indexes]\n",
    "            \n",
    "            return pd.concat([preprocessing.google_random_rows.reset_index(drop=True).add_suffix(\"_x\"), preprocessing.apple_random_rows.reset_index(drop=True).add_suffix(\"_y\")], axis=1).reset_index(drop=True)\n",
    "\n",
    "        self.false_train_data = get_false_data(self.app_store_train_data, self.google_play_train_data)\n",
    "        self.false_test_data = get_false_data(self.app_store_test_data, self.google_play_test_data)\n",
    "\n",
    "    def save_csvs(self):\n",
    "        print('save_csvs')\n",
    "\n",
    "        self.matched_test_data.to_csv(\n",
    "            \"../data/preprocessed/matched_test_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.matched_train_data.to_csv(\n",
    "            \"../data/preprocessed/matched_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "\n",
    "        self.google_play_test_data.to_csv(\n",
    "            \"../data/preprocessed/google_play_test_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.google_play_train_data.to_csv(\n",
    "            \"../data/preprocessed/google_play_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "\n",
    "        self.app_store_test_data.to_csv(\n",
    "            \"../data/preprocessed/app_store_test_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.app_store_train_data.to_csv(\n",
    "            \"../data/preprocessed/app_store_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "\n",
    "        self.false_train_data.to_csv(\n",
    "            \"../data/preprocessed/false_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.false_test_data.to_csv(\n",
    "            \"../data/preprocessed/false_test_data.csv\", index=False, header=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "add_non_processed\n",
      "preprocessing_maincategory\n",
      "preprocessing_titles\n",
      "preprocessing_author\n",
      "preprocessing_devsite\n",
      "preprocessing_description\n",
      "train_test_split\n",
      "create_false_data\n",
      "save_csvs\n",
      "CPU times: user 2min 13s, sys: 3.99 s, total: 2min 17s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessing = Preprocessing(df, True)\n",
    "preprocessing.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nselect * from app_info where store = '0' and title is not null and title <> '' and author is not null and author <> '' and description is not null and description <> '' and devsite is not null and devsite <> '' limit 1000;\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "\"\"\"\n",
    "select * from app_info where store = '0' and title is not null and title <> '' and author is not null and author <> '' and description is not null and description <> '' and devsite is not null and devsite <> '' limit 1000;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_1k = pd.read_csv(\"../data/1k_apple.csv\", low_memory=False, lineterminator='\\n')\n",
    "android_1k = pd.read_csv(\"../data/1k_android.csv\", low_memory=False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "add_non_processed\n",
      "preprocessing_maincategory\n",
      "preprocessing_titles\n",
      "preprocessing_author\n",
      "preprocessing_devsite\n",
      "preprocessing_description\n",
      "pipeline done\n",
      "CPU times: user 660 ms, sys: 31.3 ms, total: 691 ms\n",
      "Wall time: 702 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessing_apple = Preprocessing(apple_1k, False)\n",
    "preprocessing_apple.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "add_non_processed\n",
      "preprocessing_maincategory\n",
      "preprocessing_titles\n",
      "preprocessing_author\n",
      "preprocessing_devsite\n",
      "preprocessing_description\n",
      "pipeline done\n",
      "CPU times: user 699 ms, sys: 19.7 ms, total: 719 ms\n",
      "Wall time: 726 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessing_android = Preprocessing(android_1k, False)\n",
    "preprocessing_android.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_1k_processes = preprocessing_android.df_after_preprocessing\n",
    "apple_1k_processes = preprocessing_apple.df_after_preprocessing\n",
    "crossed_all_data = android_1k_processes.merge(apple_1k_processes, how=\"cross\")\n",
    "\n",
    "crossed_all_data.to_csv(\"../data/preprocessed/crossed_all_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     id_x  store_x  \\\n",
       "0                                    com.forsaliveplus.tv        0   \n",
       "1                                    com.forsaliveplus.tv        0   \n",
       "2                                    com.forsaliveplus.tv        0   \n",
       "3                                    com.forsaliveplus.tv        0   \n",
       "4                                    com.forsaliveplus.tv        0   \n",
       "...                                                   ...      ...   \n",
       "999995  com.extremesimulator.atv.quad.bike4x4.derby.at...        0   \n",
       "999996  com.extremesimulator.atv.quad.bike4x4.derby.at...        0   \n",
       "999997  com.extremesimulator.atv.quad.bike4x4.derby.at...        0   \n",
       "999998  com.extremesimulator.atv.quad.bike4x4.derby.at...        0   \n",
       "999999  com.extremesimulator.atv.quad.bike4x4.derby.at...        0   \n",
       "\n",
       "       apple_maincategory_x google_maincategory_x title_x author_x  \\\n",
       "0                       NaN         Entertainment    فرصة    forsa   \n",
       "1                       NaN         Entertainment    فرصة    forsa   \n",
       "2                       NaN         Entertainment    فرصة    forsa   \n",
       "3                       NaN         Entertainment    فرصة    forsa   \n",
       "4                       NaN         Entertainment    فرصة    forsa   \n",
       "...                     ...                   ...     ...      ...   \n",
       "999995                  NaN                 Games     atv  extreme   \n",
       "999996                  NaN                 Games     atv  extreme   \n",
       "999997                  NaN                 Games     atv  extreme   \n",
       "999998                  NaN                 Games     atv  extreme   \n",
       "999999                  NaN                 Games     atv  extreme   \n",
       "\n",
       "         devsite_x                                      description_x  \\\n",
       "0       forsa-live  ttbyq frs lyf forsa live khs bmshhd mbryt lywm...   \n",
       "1       forsa-live  ttbyq frs lyf forsa live khs bmshhd mbryt lywm...   \n",
       "2       forsa-live  ttbyq frs lyf forsa live khs bmshhd mbryt lywm...   \n",
       "3       forsa-live  ttbyq frs lyf forsa live khs bmshhd mbryt lywm...   \n",
       "4       forsa-live  ttbyq frs lyf forsa live khs bmshhd mbryt lywm...   \n",
       "...            ...                                                ...   \n",
       "999995    blogspot  play arizona snow atv quad bike derby and feel...   \n",
       "999996    blogspot  play arizona snow atv quad bike derby and feel...   \n",
       "999997    blogspot  play arizona snow atv quad bike derby and feel...   \n",
       "999998    blogspot  play arizona snow atv quad bike derby and feel...   \n",
       "999999    blogspot  play arizona snow atv quad bike derby and feel...   \n",
       "\n",
       "              id_y  store_y  apple_maincategory_y  google_maincategory_y  \\\n",
       "0        980196807        1                  6007                    NaN   \n",
       "1        980196880        1                  6000                    NaN   \n",
       "2        980196905        1                  6016                    NaN   \n",
       "3        980196941        1                  6004                    NaN   \n",
       "4        980196971        1                  6002                    NaN   \n",
       "...            ...      ...                   ...                    ...   \n",
       "999995  1112034516        1                  6016                    NaN   \n",
       "999996  1112035031        1                  6002                    NaN   \n",
       "999997  1112035423        1                  6003                    NaN   \n",
       "999998  1112035428        1                  6003                    NaN   \n",
       "999999  1112036551        1                  6017                    NaN   \n",
       "\n",
       "              title_y           author_y    devsite_y  \\\n",
       "0            timelime               niko  timelimeapp   \n",
       "1                 los    busca-ayuda.com     loslupes   \n",
       "2                芸人検定          yoshihiro  sweetdoctor   \n",
       "3           ゴルフの法則クイズ           kiyoyuki          fc2   \n",
       "4       trafikensuche  monopolverwaltung          mvg   \n",
       "...               ...                ...          ...   \n",
       "999995      攻略ニュースまとめ             hiroya          bit   \n",
       "999996        bnetfax           hangzhou     hzdracom   \n",
       "999997           muji                qup     qupworld   \n",
       "999998           muji                qup     qupworld   \n",
       "999999           租来租趣       江苏租来租趣商贸有限公司       zulzuq   \n",
       "\n",
       "                                            description_y  \n",
       "0       with timelime for iphone you can easily track ...  \n",
       "1       los lupes started to operate in  on the corner...  \n",
       "2       nannotsu \\npisutachionoshi jie hegozhao dai it...  \n",
       "3       anataha goruhunofa ze wodonokuraizhi tsutemasu...  \n",
       "4       holen sie sich jetzt die offizielle trafikensu...  \n",
       "...                                                   ...  \n",
       "999995  shi kuang pawahurupuroye qiu pawapurosakusesus...  \n",
       "999996  shi yao shi zhong guo dian xin shou ji chuan z...  \n",
       "999997  its now easy and hassle  free to take a ride b...  \n",
       "999998  muji driver is an ideal moneyearning tool for ...  \n",
       "999999  zu lai zu qu  shi ji yi xian pin pai wan ju  y...  \n",
       "\n",
       "[1000000 rows x 16 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_x</th>\n      <th>store_x</th>\n      <th>apple_maincategory_x</th>\n      <th>google_maincategory_x</th>\n      <th>title_x</th>\n      <th>author_x</th>\n      <th>devsite_x</th>\n      <th>description_x</th>\n      <th>id_y</th>\n      <th>store_y</th>\n      <th>apple_maincategory_y</th>\n      <th>google_maincategory_y</th>\n      <th>title_y</th>\n      <th>author_y</th>\n      <th>devsite_y</th>\n      <th>description_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>com.forsaliveplus.tv</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Entertainment</td>\n      <td>فرصة</td>\n      <td>forsa</td>\n      <td>forsa-live</td>\n      <td>ttbyq frs lyf forsa live khs bmshhd mbryt lywm...</td>\n      <td>980196807</td>\n      <td>1</td>\n      <td>6007</td>\n      <td>NaN</td>\n      <td>timelime</td>\n      <td>niko</td>\n      <td>timelimeapp</td>\n      <td>with timelime for iphone you can easily track ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>com.forsaliveplus.tv</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Entertainment</td>\n      <td>فرصة</td>\n      <td>forsa</td>\n      <td>forsa-live</td>\n      <td>ttbyq frs lyf forsa live khs bmshhd mbryt lywm...</td>\n      <td>980196880</td>\n      <td>1</td>\n      <td>6000</td>\n      <td>NaN</td>\n      <td>los</td>\n      <td>busca-ayuda.com</td>\n      <td>loslupes</td>\n      <td>los lupes started to operate in  on the corner...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>com.forsaliveplus.tv</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Entertainment</td>\n      <td>فرصة</td>\n      <td>forsa</td>\n      <td>forsa-live</td>\n      <td>ttbyq frs lyf forsa live khs bmshhd mbryt lywm...</td>\n      <td>980196905</td>\n      <td>1</td>\n      <td>6016</td>\n      <td>NaN</td>\n      <td>芸人検定</td>\n      <td>yoshihiro</td>\n      <td>sweetdoctor</td>\n      <td>nannotsu \\npisutachionoshi jie hegozhao dai it...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>com.forsaliveplus.tv</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Entertainment</td>\n      <td>فرصة</td>\n      <td>forsa</td>\n      <td>forsa-live</td>\n      <td>ttbyq frs lyf forsa live khs bmshhd mbryt lywm...</td>\n      <td>980196941</td>\n      <td>1</td>\n      <td>6004</td>\n      <td>NaN</td>\n      <td>ゴルフの法則クイズ</td>\n      <td>kiyoyuki</td>\n      <td>fc2</td>\n      <td>anataha goruhunofa ze wodonokuraizhi tsutemasu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>com.forsaliveplus.tv</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Entertainment</td>\n      <td>فرصة</td>\n      <td>forsa</td>\n      <td>forsa-live</td>\n      <td>ttbyq frs lyf forsa live khs bmshhd mbryt lywm...</td>\n      <td>980196971</td>\n      <td>1</td>\n      <td>6002</td>\n      <td>NaN</td>\n      <td>trafikensuche</td>\n      <td>monopolverwaltung</td>\n      <td>mvg</td>\n      <td>holen sie sich jetzt die offizielle trafikensu...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999995</th>\n      <td>com.extremesimulator.atv.quad.bike4x4.derby.at...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Games</td>\n      <td>atv</td>\n      <td>extreme</td>\n      <td>blogspot</td>\n      <td>play arizona snow atv quad bike derby and feel...</td>\n      <td>1112034516</td>\n      <td>1</td>\n      <td>6016</td>\n      <td>NaN</td>\n      <td>攻略ニュースまとめ</td>\n      <td>hiroya</td>\n      <td>bit</td>\n      <td>shi kuang pawahurupuroye qiu pawapurosakusesus...</td>\n    </tr>\n    <tr>\n      <th>999996</th>\n      <td>com.extremesimulator.atv.quad.bike4x4.derby.at...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Games</td>\n      <td>atv</td>\n      <td>extreme</td>\n      <td>blogspot</td>\n      <td>play arizona snow atv quad bike derby and feel...</td>\n      <td>1112035031</td>\n      <td>1</td>\n      <td>6002</td>\n      <td>NaN</td>\n      <td>bnetfax</td>\n      <td>hangzhou</td>\n      <td>hzdracom</td>\n      <td>shi yao shi zhong guo dian xin shou ji chuan z...</td>\n    </tr>\n    <tr>\n      <th>999997</th>\n      <td>com.extremesimulator.atv.quad.bike4x4.derby.at...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Games</td>\n      <td>atv</td>\n      <td>extreme</td>\n      <td>blogspot</td>\n      <td>play arizona snow atv quad bike derby and feel...</td>\n      <td>1112035423</td>\n      <td>1</td>\n      <td>6003</td>\n      <td>NaN</td>\n      <td>muji</td>\n      <td>qup</td>\n      <td>qupworld</td>\n      <td>its now easy and hassle  free to take a ride b...</td>\n    </tr>\n    <tr>\n      <th>999998</th>\n      <td>com.extremesimulator.atv.quad.bike4x4.derby.at...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Games</td>\n      <td>atv</td>\n      <td>extreme</td>\n      <td>blogspot</td>\n      <td>play arizona snow atv quad bike derby and feel...</td>\n      <td>1112035428</td>\n      <td>1</td>\n      <td>6003</td>\n      <td>NaN</td>\n      <td>muji</td>\n      <td>qup</td>\n      <td>qupworld</td>\n      <td>muji driver is an ideal moneyearning tool for ...</td>\n    </tr>\n    <tr>\n      <th>999999</th>\n      <td>com.extremesimulator.atv.quad.bike4x4.derby.at...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Games</td>\n      <td>atv</td>\n      <td>extreme</td>\n      <td>blogspot</td>\n      <td>play arizona snow atv quad bike derby and feel...</td>\n      <td>1112036551</td>\n      <td>1</td>\n      <td>6017</td>\n      <td>NaN</td>\n      <td>租来租趣</td>\n      <td>江苏租来租趣商贸有限公司</td>\n      <td>zulzuq</td>\n      <td>zu lai zu qu  shi ji yi xian pin pai wan ju  y...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000000 rows × 16 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "crossed_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}