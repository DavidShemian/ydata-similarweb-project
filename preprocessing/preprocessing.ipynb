{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import dateparser\n",
    "from cleanco import prepare_terms, basename\n",
    "import unidecode\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(155330, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/apps_matching_80%.csv\", low_memory=False, lineterminator='\\n')\n",
    "df = df.drop(df[df[\"description\"].isnull()].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df, isTrain):\n",
    "        self.df_apps_match = df\n",
    "        self.df_after_preprocessing = pd.DataFrame()\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def pipeline(self):\n",
    "        self.add_non_processed()\n",
    "        self.preprocessing_maincategory()\n",
    "        self.preprocessing_titles()\n",
    "        self.preprocessing_author()\n",
    "        self.preprocessing_devsite()\n",
    "        self.preprocessing_description()\n",
    "        # self.preprocessing_releasedate()\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.train_test_split()\n",
    "            self.create_false_data()\n",
    "            self.save_csvs()\n",
    "            return\n",
    "        \n",
    "\n",
    "\n",
    "        print('pipeline done')\n",
    "\n",
    "    def add_non_processed(self):\n",
    "        print('add_non_processed')\n",
    "\n",
    "        self.df_after_preprocessing[\"id\"] = self.df_apps_match[\"id\"]\n",
    "        self.df_after_preprocessing[\"store\"] = self.df_apps_match[\"store\"]\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.df_after_preprocessing[\"id_matched\"] = self.df_apps_match[\"id_matched\"]\n",
    "\n",
    "    def preprocessing_maincategory(self):\n",
    "        print('preprocessing_maincategory')\n",
    "        \n",
    "        maincategory = pd.read_json('maincategory.json')\n",
    "        # Change from apple catagories ids to string catagories\n",
    "\n",
    "        self.df_after_preprocessing[\"apple_maincategory\"] = (\n",
    "            self.df_apps_match[self.df_apps_match[\"store\"] == 1]\n",
    "            .loc[:, \"maincategory\"]\n",
    "            .replace(\n",
    "                maincategory['apple']['numbered'],\n",
    "                maincategory['apple']['labeled'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Change from google play catagories to apple catagories\n",
    "        self.df_after_preprocessing[\"google_maincategory\"] = (\n",
    "            self.df_apps_match[self.df_apps_match[\"store\"] == 0]\n",
    "            .loc[:, \"maincategory\"]\n",
    "            .replace(\n",
    "                maincategory['google']['upper'],\n",
    "                maincategory['google']['lower'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def preprocessing_titles(self):\n",
    "        print('preprocessing_titles')\n",
    "\n",
    "        # lower case the titles and seperate the title\n",
    "        def create_title(titles):\n",
    "            # todo: ask davis if need it also for athuor\n",
    "            return [\n",
    "                title.lower()\n",
    "                .strip()\n",
    "                .partition(\":\")[0]\n",
    "                .partition(\"-\")[0]\n",
    "                .partition(\" \")[0]\n",
    "                for title in titles\n",
    "            ]\n",
    "\n",
    "        self.df_after_preprocessing[\"title\"] = create_title(self.df_apps_match[\"title\"])\n",
    "\n",
    "    def preprocessing_author(self):\n",
    "        print('preprocessing_author')\n",
    "\n",
    "        def create_author(authors):\n",
    "            terms = prepare_terms()\n",
    "            # Running twice in order to remove multiple endings, i.e Co., Ltd.\n",
    "            authors = [\n",
    "                basename(\n",
    "                    author.lower().strip(), terms, prefix=True, middle=True, suffix=True\n",
    "                )\n",
    "                for author in authors\n",
    "            ]\n",
    "            authors = [\n",
    "                basename(\n",
    "                    author, terms, prefix=True, middle=True, suffix=True\n",
    "                ).partition(\" \")[0]\n",
    "                for author in authors\n",
    "            ]\n",
    "            return authors\n",
    "\n",
    "        self.df_after_preprocessing[\"author\"] = create_author(self.df_apps_match[\"author\"])\n",
    "    \n",
    "    def preprocessing_devsite(self):\n",
    "        print('preprocessing_devsite')\n",
    "\n",
    "        def create_devsite(devsites):\n",
    "            return [\n",
    "                tldextract.extract(devsite.lower().strip()).domain\n",
    "                for devsite in devsites\n",
    "            ]\n",
    "\n",
    "        self.df_after_preprocessing[\"devsite\"] = create_devsite(self.df_apps_match[\"devsite\"].values.astype(str))\n",
    "\n",
    "    def preprocessing_releasedate(self):\n",
    "        print('preprocessing_releasedate')\n",
    "\n",
    "        def parse_date(date):\n",
    "            if not isinstance(date, str):\n",
    "                # always nan values\n",
    "                return\n",
    "\n",
    "            return dateparser.parse(date)\n",
    "\n",
    "        self.google_play_df_after_eda[\"releasedate\"] = pd.to_datetime(\n",
    "            self.google_play_df[\"releasedate\"].apply(parse_date), errors=\"coerce\"\n",
    "        )\n",
    "        # self.google_play_df['releasedate'].apply(parse_date).values.astype('datetime64[D]')\n",
    "        self.app_store_df_after_eda[\"releasedate\"] = pd.to_datetime(\n",
    "            self.app_store_df[\"releasedate\"].apply(parse_date), errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    def preprocessing_description(self):  # todo: make it better..\n",
    "        print('preprocessing_description')\n",
    "\n",
    "        def create_descriptions(descriptions):\n",
    "            return [\n",
    "                unidecode.unidecode(re.sub(r\"\\d+\", \"\", description))\n",
    "                .lower()\n",
    "                .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "                .strip()\n",
    "                for description in descriptions\n",
    "            ]\n",
    "\n",
    "        def save_tfidf_embeddings(documents):\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            embeddings = vectorizer.fit_transform(documents)\n",
    "            scipy.sparse.save_npz('../data/tfidf/embeddings.npz', embeddings)\n",
    "            \n",
    "\n",
    "        self.df_after_preprocessing[\"description\"] = create_descriptions(self.df_apps_match[\"description\"])\n",
    "        save_tfidf_embeddings(self.df_after_preprocessing[\"description\"])\n",
    "\n",
    "    def train_test_split(self):\n",
    "        print('train_test_split')\n",
    "        \n",
    "        # Shuffle dataset \n",
    "        shuffle_df = self.df_after_preprocessing.sample(frac=1)\n",
    "\n",
    "        # get 10% of data\n",
    "        test_size = int(0.1 * len(self.df_after_preprocessing))\n",
    "\n",
    "        test_set_first_part = shuffle_df[:test_size]\n",
    "        test_set_second_part = self.df_after_preprocessing[self.df_after_preprocessing[\"id_matched\"].isin(test_set_first_part[\"id\"])]\n",
    "\n",
    "        self.test_data = pd.concat([test_set_first_part, test_set_second_part])\n",
    "        self.train_data = self.df_after_preprocessing[~self.df_after_preprocessing[\"id\"].isin(self.test_data[\"id\"])]\n",
    "\n",
    "        self.google_play_test_data = self.test_data[self.test_data[\"store\"] == 0].rename(columns={'google_maincategory': 'maincategory'}).drop(columns=['store', 'apple_maincategory']).reset_index(drop=True)\n",
    "        \n",
    "        self.google_play_train_data = self.train_data[self.train_data[\"store\"] == 0].rename(columns={'google_maincategory': 'maincategory'}).drop(columns=['store', 'apple_maincategory']).reset_index(drop=True)\n",
    "\n",
    "        self.app_store_test_data = self.test_data[self.test_data[\"store\"] == 1].rename(columns={'apple_maincategory': 'maincategory'}).drop(columns=['store', 'google_maincategory']).reset_index(drop=True)\n",
    "\n",
    "        self.app_store_train_data = self.train_data[self.train_data[\"store\"] == 1].rename(columns={'apple_maincategory': 'maincategory'}).drop(columns=['store', 'google_maincategory']).reset_index(drop=True)\n",
    "\n",
    "        self.matched_test_data = self.test_data.merge(self.test_data, how=\"inner\", left_on=\"id\", right_on=\"id_matched\").reset_index(drop=True)\n",
    "        self.matched_train_data = self.train_data.merge(self.train_data, how=\"inner\", left_on=\"id\", right_on=\"id_matched\").reset_index(drop=True)\n",
    "\n",
    "        # remove duplicate matches \n",
    "        mask_test = self.matched_test_data[self.matched_test_data[\"store_x\"] == 1].index\n",
    "        self.matched_test_data.drop(mask_test, inplace=True)\n",
    "\n",
    "        mask_train = self.matched_train_data[self.matched_train_data[\"store_x\"] == 1].index\n",
    "        self.matched_train_data.drop(mask_train, inplace=True)\n",
    "\n",
    "        # remove unmatched apps\n",
    "        # TODO: check how come we have unmatched apps\n",
    "        self.matched_test_data = self.matched_test_data.dropna(subset=[\"id_y\"])\n",
    "        self.matched_train_data = self.matched_train_data.dropna(subset=[\"id_y\"])\n",
    "\n",
    "\n",
    "    def create_false_data(self):\n",
    "        print('create_false_data')\n",
    "\n",
    "        def get_false_data(apple_train_data, google_train_data):\n",
    "            num_of_matches = len(apple_train_data) if len(apple_train_data) % 2 == 0 else len(apple_train_data) - 1 # keeping it even\n",
    "            sample_size = int(num_of_matches * 5)\n",
    "\n",
    "            google_rand_indexes = np.random.randint(num_of_matches, size=int(sample_size / 2))\n",
    "            apple_rand_indexes = np.random.randint(num_of_matches, size=int(sample_size / 2))\n",
    "\n",
    "            self.google_random_rows = google_train_data.iloc[google_rand_indexes]\n",
    "            self.apple_random_rows = apple_train_data.iloc[apple_rand_indexes]\n",
    "            \n",
    "            return pd.concat([preprocessing.google_random_rows.reset_index(drop=True).add_suffix(\"_x\"), preprocessing.apple_random_rows.reset_index(drop=True).add_suffix(\"_y\")], axis=1).reset_index(drop=True)\n",
    "\n",
    "        self.false_train_data = get_false_data(self.app_store_train_data, self.google_play_train_data)\n",
    "        self.false_test_data = get_false_data(self.app_store_test_data, self.google_play_test_data)\n",
    "\n",
    "    def save_csvs(self):\n",
    "        print('save_csvs')\n",
    "\n",
    "        self.matched_test_data.to_csv(\n",
    "            \"../data/preprocessed/matched_test_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.matched_train_data.to_csv(\n",
    "            \"../data/preprocessed/matched_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "\n",
    "        self.google_play_test_data.to_csv(\n",
    "            \"../data/preprocessed/google_play_test_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.google_play_train_data.to_csv(\n",
    "            \"../data/preprocessed/google_play_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "\n",
    "        self.app_store_test_data.to_csv(\n",
    "            \"../data/preprocessed/app_store_test_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.app_store_train_data.to_csv(\n",
    "            \"../data/preprocessed/app_store_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "\n",
    "        self.false_train_data.to_csv(\n",
    "            \"../data/preprocessed/false_train_data.csv\", index=False, header=True\n",
    "        )\n",
    "        self.false_test_data.to_csv(\n",
    "            \"../data/preprocessed/false_test_data.csv\", index=False, header=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "add_non_processed\n",
      "preprocessing_maincategory\n",
      "preprocessing_titles\n",
      "preprocessing_author\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessing = Preprocessing(df, True)\n",
    "preprocessing.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "select * from app_info where store = '0' and title is not null and title <> '' and author is not null and author <> '' and description is not null and description <> '' and devsite is not null and devsite <> '' limit 1000;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_1k = pd.read_csv(\"../data/1k_apple.csv\", low_memory=False, lineterminator='\\n')\n",
    "android_1k = pd.read_csv(\"../data/1k_android.csv\", low_memory=False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocessing_apple = Preprocessing(apple_1k, False)\n",
    "preprocessing_apple.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocessing_android = Preprocessing(android_1k, False)\n",
    "preprocessing_android.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_1k_processes = preprocessing_android.df_after_preprocessing\n",
    "apple_1k_processes = preprocessing_apple.df_after_preprocessing\n",
    "crossed_all_data = android_1k_processes.merge(apple_1k_processes, how=\"cross\")\n",
    "\n",
    "crossed_all_data.to_csv(\"../data/preprocessed/crossed_all_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_all_data"
   ]
  }
 ]
}